# MÃ³dulo AvanÃ§ado de PrÃ©-processamento de Texto

Este documento descreve o mÃ³dulo de prÃ©-processamento de texto desenvolvido para o projeto `finetuning-energy`, seguindo as boas prÃ¡ticas de engenharia de software para projetos de IA com PyTorch e HuggingFace.

## ğŸ“‹ VisÃ£o Geral

O mÃ³dulo `text_preprocessing_advanced.py` converte datasets em formato Excel para um formato estruturado, compatÃ­vel com a biblioteca HuggingFace Datasets e adequado para tarefas de sumarizaÃ§Ã£o automÃ¡tica.

### ğŸ¯ CaracterÃ­sticas Principais

- **Formato Estruturado**: CompatÃ­vel com bibliotecas de processamento de texto
- **IntegraÃ§Ã£o HuggingFace**: Datasets prontos para usar com transformers
- **Limpeza AvanÃ§ada**: CorreÃ§Ã£o automÃ¡tica de problemas de encoding
- **Splits AutomÃ¡ticos**: DivisÃ£o em treino, validaÃ§Ã£o e teste
- **MÃºltiplos Formatos**: SaÃ­da em JSON, Parquet e CSV
- **ValidaÃ§Ã£o Robusta**: VerificaÃ§Ã£o de qualidade dos dados
- **ConfiguraÃ§Ã£o FlexÃ­vel**: ParÃ¢metros ajustÃ¡veis para diferentes cenÃ¡rios

## ğŸš€ Uso RÃ¡pido

### MÃ©todo Simples (Recomendado)

```python
from src.text_preprocessing_advanced import process_excel_to_dataset

# Processamento completo com configuraÃ§Ã£o padrÃ£o
result = process_excel_to_dataset("data/dataset.xlsx")

if result['success']:
    print(f"âœ… Dataset processado: {result['dataset_info']['total_examples']} exemplos")
    print(f"Arquivos salvos: {result['saved_files']}")
```

### MÃ©todo AvanÃ§ado (Controle Total)

```python
from src.text_preprocessing_advanced import AdvancedTextProcessor, TextPreprocessingConfig

# ConfiguraÃ§Ã£o customizada
config = TextPreprocessingConfig(
    text_column="Texto",
    summary_column="Resumo", 
    title_column="Processo",
    min_text_length=100,
    max_text_length=4000,
    test_size=0.15,
    validation_size=0.1,
    additional_columns=["Legislacao", "Pareceres"],
    save_formats=['json', 'parquet']
)

# Processamento passo a passo
processor = AdvancedTextProcessor(config)
processor.load_excel("data/dataset.xlsx")
processed_data = processor.preprocess_data()
dataset_dict = processor.create_dataset_splits()
validation = processor.validate_dataset_format()
saved_files = processor.save_dataset()
```

## ğŸ“ Estrutura de Arquivos

```
```
src/
â”œâ”€â”€ text_preprocessing_advanced.py    # MÃ³dulo principal
â””â”€â”€ pipeline_base.py                 # Base para pipelines (se existir)

tests/
â””â”€â”€ test_text_preprocessing_advanced.py  # Testes unitÃ¡rios

examples/
â””â”€â”€ example_text_preprocessing.py    # DemonstraÃ§Ã£o completa
```

data/
â”œâ”€â”€ dataset.xlsx                     # Dataset original
â””â”€â”€ processed/                       # Dados processados
    â”œâ”€â”€ dataset_simple.json         # Formato JSON simples
    â”œâ”€â”€ dataset_train.parquet       # Split de treino
    â”œâ”€â”€ dataset_validation.parquet  # Split de validaÃ§Ã£o
    â””â”€â”€ dataset_test.parquet        # Split de teste
```

## âš™ï¸ ConfiguraÃ§Ã£o

### TextPreprocessingConfig

```python
@dataclass
class TextPreprocessingConfig:
    # Mapeamento de colunas
    text_column: str = "Texto"           # Campo principal
    summary_column: str = "Resumo"       # Campo de resumo
    title_column: str = "Processo"       # Campo de tÃ­tulo
    
    # Colunas adicionais
    additional_columns: List[str] = []   # Campos extras
    
    # Limpeza de texto
    clean_text: bool = True
    normalize_unicode: bool = True
    remove_extra_whitespace: bool = True
    fix_encoding_issues: bool = True
    
    # Filtros de qualidade
    min_text_length: int = 500
    max_text_length: int = 50000
    min_summary_length: int = 100
    max_summary_length: int = 5000
    
    # DivisÃ£o dos dados
    test_size: float = 0.2              # 20% para teste
    validation_size: float = 0.1        # 10% para validaÃ§Ã£o
    random_state: int = 42
    
    # SaÃ­da
    output_dir: str = "data/processed"
    save_formats: List[str] = ['json', 'parquet']
```

## ğŸ”§ Comandos Make

```bash
# DemonstraÃ§Ã£o completa
make demo-preprocessing-advanced

# Processamento do dataset
make process-dataset-advanced

# ValidaÃ§Ã£o dos dados processados
make validate-advanced-dataset

# EstatÃ­sticas do dataset
make advanced-stats

# Testes unitÃ¡rios
make test-preprocessing-advanced
```

## ğŸ“Š Formato de SaÃ­da

### Formato Estruturado CompatÃ­vel

```json
{
  "text": [
    "Texto completo do documento 1...",
    "Texto completo do documento 2..."
  ],
  "summary": [
    "Resumo do documento 1...",
    "Resumo do documento 2..."
  ],
  "title": [
    "TÃ­tulo/ID do documento 1",
    "TÃ­tulo/ID do documento 2"
  ]
}
```

### Splits Criados

- **train**: 70% dos dados (apÃ³s remover teste)
- **validation**: 10% dos dados de treino
- **test**: 20% dos dados totais

## ğŸ§ª Testes e ValidaÃ§Ã£o

### Testes UnitÃ¡rios

```bash
# Executar todos os testes
pytest tests/test_text_preprocessing_advanced.py -v

# Executar testes especÃ­ficos
pytest tests/test_text_preprocessing_advanced.py::TestTextCleaner -v
```

### ValidaÃ§Ã£o AutomÃ¡tica

O mÃ³dulo inclui validaÃ§Ã£o automÃ¡tica que verifica:

- âœ… PresenÃ§a das features obrigatÃ³rias (`text`, `summary`, `title`)
- âœ… Tipos de dados corretos
- âœ… Qualidade dos dados (textos vazios, comprimentos)
- âœ… Integridade dos splits

## ğŸ” Limpeza de Texto

### Problemas Corrigidos Automaticamente

- **Encoding Issues**: `_x000D_`, `_x000A_`, `_x0009_`
- **Unicode**: NormalizaÃ§Ã£o NFKC
- **EspaÃ§os**: RemoÃ§Ã£o de espaÃ§os extras e quebras mÃºltiplas
- **Caracteres Especiais**: Limpeza de caracteres mal codificados

### Exemplo de Limpeza

```python
# Antes
texto_sujo = "PLENÃRIO _x000D_\n_x000D_\nTEXTO    com   espaÃ§os"

# Depois  
texto_limpo = "PLENÃRIO\n\nTEXTO com espaÃ§os"
```

## ğŸ”— IntegraÃ§Ã£o com HuggingFace

### Carregamento Direto

```python
from datasets import load_from_disk

# Carregar dataset processado
dataset = load_from_disk("data/processed/dataset_structured_format")

# Verificar estrutura
print("Splits:", list(dataset.keys()))
print("Features:", list(dataset['train'].features.keys()))
print("Exemplo:", dataset['train'][0])
```

### Uso com Transformers

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# Carregar modelo para sumarizaÃ§Ã£o
tokenizer = AutoTokenizer.from_pretrained("facebook/bart-base")
model = AutoModelForSeq2SeqLM.from_pretrained("facebook/bart-base")

# Tokenizar dataset
def tokenize_function(examples):
    inputs = tokenizer(examples['text'], truncation=True, padding=True)
    labels = tokenizer(examples['summary'], truncation=True, padding=True)
    inputs['labels'] = labels['input_ids']
    return inputs

tokenized_dataset = dataset.map(tokenize_function, batched=True)
```

## ğŸ“ˆ EstatÃ­sticas de Exemplo

Para o dataset atual (510 registros):

```
Total de exemplos: 507 (3 removidos por filtros)

Splits:
â”œâ”€â”€ train: 364 exemplos (72%)
â”œâ”€â”€ validation: 41 exemplos (8%)
â””â”€â”€ test: 102 exemplos (20%)

EstatÃ­sticas por campo:
â”œâ”€â”€ Texto: 500-50000 caracteres (mÃ©dia: ~5200)
â”œâ”€â”€ Resumo: 100-5000 caracteres (mÃ©dia: ~1400)
â””â”€â”€ TÃ­tulo: ID do processo (ex: "112962-5/2024")
```

## ğŸš¨ ResoluÃ§Ã£o de Problemas

### Erro: "Colunas obrigatÃ³rias nÃ£o encontradas"

```python
# Verificar colunas disponÃ­veis
df = pd.read_excel("data/dataset.xlsx")
print("Colunas:", list(df.columns))

# Ajustar configuraÃ§Ã£o
config = TextPreprocessingConfig(
    text_column="sua_coluna_texto",
    summary_column="sua_coluna_resumo", 
    title_column="sua_coluna_titulo"
)
```

### Erro: "Todos os registros foram filtrados"

```python
# Ajustar filtros de comprimento
config = TextPreprocessingConfig(
    min_text_length=50,      # Reduzir mÃ­nimo
    max_text_length=100000,  # Aumentar mÃ¡ximo
    min_summary_length=10,   # Reduzir mÃ­nimo
    max_summary_length=10000 # Aumentar mÃ¡ximo
)
```

### Problema de MemÃ³ria

```python
# Processar em lotes menores (implementaÃ§Ã£o futura)
# Usar formatos mais eficientes
config = TextPreprocessingConfig(
    save_formats=['parquet']  # Apenas Parquet (mais eficiente)
)
```

## ğŸ”„ IntegraÃ§Ã£o com Pipeline Principal

Para integrar com o sistema de fine-tuning existente:

```python
from src.text_preprocessing_advanced import process_excel_to_dataset
from src.finetuning import LlamaFineTuner

# 1. Processar dados
result = process_excel_to_dataset("data/dataset.xlsx")

# 2. Usar no fine-tuning
if result['success']:
    # Carregar dataset processado
    dataset = load_from_disk("data/processed/dataset_structured_format")
    
    # Configurar fine-tuning
    finetuner = LlamaFineTuner()
    finetuner.load_dataset_from_dict(dataset)
    finetuner.train()
```

## ğŸ“ PrÃ³ximos Passos

1. **OtimizaÃ§Ã£o de Performance**: Processamento em paralelo para datasets grandes
2. **Mais Formatos**: Suporte para CSV, TSV, JSON nativo
3. **ValidaÃ§Ã£o AvanÃ§ada**: DetecÃ§Ã£o de duplicatas e qualidade semÃ¢ntica
4. **Metrics**: EstatÃ­sticas mais detalhadas (perplexidade, diversidade)
5. **ConfiguraÃ§Ãµes Predefinidas**: Templates para diferentes tipos de tarefa

## ğŸ“„ LicenÃ§a

Este mÃ³dulo faz parte do projeto `finetuning-energy` e segue a mesma licenÃ§a do projeto principal.
