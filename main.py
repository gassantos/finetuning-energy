from src.finetuning import LlamaFineTuner
from src.text_preprocessing_advanced import process_excel_to_dataset
from config.config import settings
from pathlib import Path
import logging

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def preprocess_dataset():
    """Pr√©-processa o dataset Excel para formato de sumariza√ß√£o"""
    logger.info("Iniciando pr√©-processamento do dataset...")
    
    # Configurar caminhos
    excel_file = Path("data/dataset.xlsx")
    output_dir = Path("data/processed")
    
    # Verificar se arquivo existe
    if not excel_file.exists():
        raise FileNotFoundError(f"Dataset n√£o encontrado: {excel_file}")
    
    # Configura√ß√µes de pr√©-processamento para sumariza√ß√£o
    config_params = {
        "text_column": "Texto",
        "summary_column": "Resumo", 
        "title_column": "Processo",
        "min_text_length": 100,        # Textos muito curtos n√£o s√£o √∫teis para sumariza√ß√£o
        "max_text_length": 8000,       # Limite para modelos de linguagem
        "min_summary_length": 20,      # Resumos muito curtos n√£o s√£o informativos
        "max_summary_length": 1000,    # Resumos muito longos perdem o prop√≥sito
        "test_size": 0.15,             # 15% para teste
        "validation_size": 0.10,       # 10% para valida√ß√£o 
        "clean_text": True,            # Ativar limpeza de texto
        "save_formats": ["json", "parquet"]  # Formatos compat√≠veis com HF
    }
    
    try:
        # Executar pr√©-processamento
        result = process_excel_to_dataset(
            excel_file=str(excel_file),
            output_dir=str(output_dir),
            **config_params
        )
        
        # Verificar resultado
        if result["success"] and result["validation"]["valid"]:
            logger.info("‚úÖ Pr√©-processamento conclu√≠do com sucesso!")
            logger.info(f"Total de exemplos: {result['dataset_info']['total_examples']}")
            logger.info(f"Splits criados: {list(result['dataset_info']['splits'].keys())}")
            logger.info(f"Arquivos salvos: {result['saved_files']}")
            
            # Retornar caminho do dataset processado
            dataset_path = output_dir / "dataset_structured_format"
            return str(dataset_path)
        else:
            raise RuntimeError("Falha na valida√ß√£o do dataset processado")
            
    except Exception as e:
        logger.error(f"Erro no pr√©-processamento: {e}")
        raise


def main():
    """Fun√ß√£o principal para executar o pr√©-processamento e fine-tuning com monitoramento energ√©tico"""
    logger.info("=== Iniciando Pipeline Completo: Pr√©-processamento + Fine-tuning ===")
    
    try:
        # Etapa 1: Pr√©-processamento do dataset
        logger.info("üîÑ Etapa 1: Pr√©-processamento do dataset")
        dataset_path = preprocess_dataset()
        logger.info(f"Dataset processado dispon√≠vel em: {dataset_path}")
        
        # Etapa 2: Configura√ß√£o do fine-tuning
        logger.info("üîÑ Etapa 2: Configura√ß√£o do fine-tuning")
        fine_tuner = LlamaFineTuner(str(settings.WANDB_KEY), str(settings.HF_TOKEN))
        
        # Etapa 3: Executar fine-tuning
        logger.info("üîÑ Etapa 3: Executando fine-tuning com monitoramento energ√©tico")
        trainer = fine_tuner.run_complete_pipeline(dataset_path=dataset_path)
        
        logger.info("‚úÖ Pipeline completo executado com sucesso!")
        return trainer
        
    except Exception as e:
        logger.error(f"‚ùå Erro no pipeline: {e}")
        raise


if __name__ == "__main__":
    trainer = main()
